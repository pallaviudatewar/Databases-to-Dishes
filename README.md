# Databases-to-Dishes
Report Format
You are free to format your report but a good format is to follow a short paper: 
1. Introduction
2. Methods
3. Experiments 
4. Results&Discussion
5. Conclusions.

We will also add sections or write under a section in any section mentioned above
1. Challenges faced
2. Application of the product
3. How to evaluate our response/output
4. Why did we use :
   i) The generator which we are currently using
   ii) The retriever which we are cuurently using

Marks are given for
i) Novelty
ii) Delivering our project proposal
iii) Analysis of our results


Challenges encountered:
-Evaluation of our recipe?
RLHF
-Mixture of two ingredients that don’t go together and give stomach ache?
DISCLAIMER : You should go through the ingredients list and not rely on AI generated recipes.

Show comparison of our recipe generated using our retreiver vs the recipe generated by the LLM
One recipe will be generated by the LLM(without any knowledge base). One recipe will be generated by our architecture. How do you show our recipe is better
- Evaluation can be done using RLHF to compare recipes generated by generator only and generator embedded with the retriever and comparison can be done by qualified individuals to compare which recipe is superior.

Comparison of Generator Models - Why did you choose X generator:
We experimented with different generator models such as 
GPT-2
Llama-2
GPT-3.5/4 (ChatGpt/ChatGPT Pro)
GPT 4 is a powerful generator model for x,y,z reasons.

Why Contextual Compression
https://blog.langchain.dev/improving-document-retrieval-with-contextual-compression/

Task -  to retrieve
Suppose you wanted to create a chatbot that could answer questions about your personal notes. One simple approach is to embed your notes in equally-sized chunks and store the embeddings in a vector store. When you ask the system a question, it embeds your question, performs a similarity search over the vector store, retrieves the most relevant documents (chunks of text), and appends them to the LLM prompt

Problem
One problem with this approach is that when you ingest data into your document storage system, you often don’t know what specific queries will be used to retrieve those documents. In our notes Q&A example, we simply partitioned our text into equally-sized chunks. That means that when we get a specific user question and retrieve a document, even if the document has some relevant text it likely has some irrelevant text as well.
Inserting irrelevant information into the LLM prompt is bad because:
It might distract the LLM from the relevant information
It takes up precious space that could be used to insert other relevant information.

Solution
To help with this we’ve introduced a DocumentCompressor abstraction which allows you to run compress_documents(documents: List[Document], query: str) on your retrieved documents. The idea is simple: instead of immediately returning retrieved documents as-is, we can compress them using the context of the given query so that only the relevant information is returned. “Compressing” here refers to both compressing the contents of an individual document and filtering out documents wholesale.
Retrieval Q&A system with contextual document compression
The goal of compressors is to make it easy to pass only the relevant information to the LLM. By doing this, it also enables you to pass along more information to the LLM, since in the initial retrieval step you can focus on recall (e.g. by increasing the number of documents returned) and let the compressors handle precision.


How do we model substitutes?
Ans: 
i) Tuple grouping : Group all similar ingredients/ substitutes in a tuple and ask our LLM to prompt ingredients from same tuples for example:
(Honey, (Sugar+Water), Saccharin, Steria) are similar elements/ substitutes (Chilli Powder, Paprika, Cajun Pepper)
ii) Semantic Matching : - Sentence BERT, Poly Encoders, Siamese Neural Networks(Training is expensive).
iii) Metric Learning : In the context of deep learning, triplet loss is a method for training neural networks to produce embeddings where the distance between similar elements is minimized, and the distance between dissimilar elements is maximized.

A unique feature of human intelligence is the ability to seamlessly combine task-oriented actions with verbal reasoning. Consider the example of cooking up a dish in the kitchen. Between any two specific actions, we may reason in language in order to track progress (“now that everything is cut, I should heat up the pot of water”), to handle exceptions or adjust the plan according to the situation (“I don’t have salt, so let me use soy sauce and pepper instead”), and to realize when external information is needed (“how do I prepare dough? Let me search on the Internet”).
The important point of the above quote (and in fact the paper) is the intention to combine two powerful abilities of LLMs — reasoning (e.g. chain-of-thought prompting) and acting (e.g. action plan generation). While the former helps with improving the accuracy of an individual task, the latter provides the LLM power to perform multiple tasks. The plan is quite simple — ask LLM a question (input) and let it “plan” what to do next (action) by reasoning on the input (thoughts). It can even propose values to the action (action input). Once we perform the action and get an output (observation) the LLM can again reason (thought) to propose the next “plan” (action). In a way, we keep the LLM busy in this loop, until it terminates and we get the result we wanted. To summarize we iterate over “input → thoughts → action → action input → observation → thoughts”.


